{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현하려는 것\n",
    "# https://github.com/vinhkhuc/MemN2N-babi-python\n",
    "# https://github.com/nmhkahn/MemN2N-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "data_dir = glob.glob(os.getcwd() + '/data/')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cd /home/hyungjunkim/Dropbox/repo/dl/MemN2N/scripts/data && \\  \n",
    "wget https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz && \\  \n",
    "tar xvf babi_tasks_1-20_v1-2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_data = bAbIDataset(data_dir + 'tasks_1-20_v1-2/en/', task_id=task_id, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "test_data = bAbIDataset(data_dir + 'tasks_1-20_v1-2/en/', task_id=task_id, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_re = {value:key for key, value in train_data.word_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'back',\n",
       " 2: 'bathroom',\n",
       " 3: 'bedroom',\n",
       " 4: 'daniel',\n",
       " 5: 'garden',\n",
       " 6: 'hallway',\n",
       " 7: 'is',\n",
       " 8: 'john',\n",
       " 9: 'journeyed',\n",
       " 10: 'kitchen',\n",
       " 11: 'mary',\n",
       " 12: 'moved',\n",
       " 13: 'office',\n",
       " 14: 'sandra',\n",
       " 15: 'the',\n",
       " 16: 'to',\n",
       " 17: 'travelled',\n",
       " 18: 'went',\n",
       " 19: 'where'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 12 16 15  2 11]\n",
      "mary moved to the bathroom mary\n",
      "[ 8 18 16 15  6 10]\n",
      "john went to the hallway kitchen\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "[0 0 0 0 0 0]\n",
      "\n",
      "where is mary\n",
      "bathroom\n"
     ]
    }
   ],
   "source": [
    "for x,y,z in train_data:\n",
    "    for idx in x:\n",
    "        print (idx)\n",
    "        print (\" \".join([word_re.get(int(i), '.') for i in idx if int(i) != 0]))\n",
    "    \n",
    "    print (\" \".join([word_re.get(int(i), '?') for i in y if int(i) != 0]))\n",
    "    print (word_re.get(int(z), '?'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = train_data.data_story, train_data.data_query, train_data.data_answer\n",
    "inputs_test, queries_test, answers_test = test_data.data_story, test_data.data_query, test_data.data_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(sentences, emb_dim):\n",
    "    s_tmp = sentences.asnumpy()\n",
    "    n_row,n_col = np.shape(s_tmp)\n",
    "    for i in range(n_row):\n",
    "        for j in range(n_col):\n",
    "            s_tmp[i,j] = (1-j/n_col) - (n_row/emb_dim)*(1-2*j/n_col)    \n",
    "    return mx.nd.array(s_tmp, ctx).expand_dims(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-To-End Memory Network (N2NMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i = 문장   \n",
    "j = 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: $x_{i}$: 문장(context)  \n",
    "$q$: 질의  \n",
    "$target$: 답변(a word or words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img0](imgs/n2nmn_fig0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메모리 벡터: 임베딩된 워드의 합을 문장으로 표기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_{i}={\\sum}_{j}Ax_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention $P_{i}$: 질의(q)와 가장 관련된 Senence(context) 확률로 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_i=Softmax(uT {\\cdot} mi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o={\\sum}_{i}p_{i}c_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a=Softmax(W(o+u))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internal state $u={\\sum}_{j}Bq_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습: A, C, B, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjacent: output embedding(o1 + u1)이 새로운 input embedding이 됨   \n",
    "Layer-wise (RNN-like): 모든 레이어가 A와 C를 공유함.  $uk^{+1} = Hu^{k}+o^{k}$, linear mapping H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능 올리기: 1. Senetence Representation, 2. Terporal Encoding, 3. Lerning time invariance by injection random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10, 6)\n",
      "(1000, 6)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(inputs_train))\n",
    "print (np.shape(queries_train))\n",
    "print (np.shape(answers_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd, init\n",
    "from mxnet.gluon import Block, nn\n",
    "\n",
    "\n",
    "class MemN2N(Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(MemN2N, self).__init__(**kwargs)\n",
    "\n",
    "        ## 고정\n",
    "        self.vocab_size = len(word_re)\n",
    "        \n",
    "        ## \n",
    "        self.init_std = 0.015\n",
    "        self.nhop = 3\n",
    "        self.emb_dim = 50\n",
    "\n",
    "        with self.name_scope():\n",
    "            \n",
    "            # Embedding A (메모리 벡터)\n",
    "            self.A = nn.Embedding(input_dim=self.vocab_size, \n",
    "                                  output_dim=self.emb_dim, \n",
    "                                  weight_initializer=init.Normal(self.init_std))\n",
    "             \n",
    "            # Embedding C (컨텍스트 벡터)\n",
    "            self.C = nn.Embedding(input_dim=self.vocab_size,\n",
    "                                  output_dim=self.emb_dim, \n",
    "                                  weight_initializer=init.Normal(self.init_std))\n",
    "            \n",
    "            # Embedding Q (쿼리 벡터)\n",
    "            self.B = nn.Embedding(input_dim=self.vocab_size, \n",
    "                                  output_dim=self.emb_dim, \n",
    "                                  weight_initializer=init.Normal(self.init_std))\n",
    "\n",
    "            # Final Predict \n",
    "            self.W = nn.Dense(self.vocab_size, weight_initializer=init.Normal(self.init_std))\n",
    "            \n",
    "    def forward(self, sentences, question):\n",
    "\n",
    "        hid = []\n",
    "        hid.append(question)\n",
    "        \n",
    "        # 16, 50, 7 , 19 > 16, 50, 19\n",
    "        m_i = self.A(sentences) #* position_encoding(sentences, self.emb_dim)\n",
    "        m_i = m_i.sum(axis=2)\n",
    "        \n",
    "        # 16, 50, 7 , 19 > 16, 50, 19\n",
    "        c_i = self.C(sentences)\n",
    "        c_i = c_i.sum(axis=2)\n",
    "            \n",
    "        for n in range(self.nhop):\n",
    "            \n",
    "            if n == 0:\n",
    "                #  16, 7, 50 > 16, 50\n",
    "                u = self.B(hid[-1])# * position_encoding(hid[-1], self.emb_dim)\n",
    "                u = u.sum(axis=1)\n",
    "            else:\n",
    "                u = hid[-1]\n",
    "\n",
    "            # [16, 1, 50] [16, 50, 10]  > [16, 1, 50]\n",
    "            m_out = nd.batch_dot(u.expand_dims(1), m_i.swapaxes(1,2))\n",
    "            P = nd.softmax(m_out, axis=2)\n",
    "            \n",
    "            #Cout = P * c_i\n",
    "            # [16, 1, 10] [16, 10, 50] > [16, 1, 50]\n",
    "            Cout = nd.batch_dot(P,  c_i)\n",
    "            # [16, 1, 50] > [16, 50]\n",
    "            o = Cout.reshape((0,-1))#Cout.sum(axis=(1))\n",
    "            \n",
    "            ## O + U [16, 50]\n",
    "            Dout = o + u\n",
    "            hid.append(Dout)\n",
    "           \n",
    "        z = self.W(hid[-1])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainer, x_input, x_query, x_answers, batch_size):\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    max_grad_norm=40\n",
    "    N = int(math.ceil(len(x_input) / batch_size))\n",
    "    cost = 0.0\n",
    "\n",
    "    for idx in range(N):\n",
    "        with autograd.record():\n",
    "            out = model(x_input, x_query)\n",
    "            loss = softmax_cross_entropy(out, x_answers)\n",
    "            loss.backward()\n",
    "            \n",
    "        grads = [i.grad() for i in model.collect_params().values()]\n",
    "        gluon.utils.clip_global_norm(grads, max_grad_norm)\n",
    "        trainer.step(batch_size)\n",
    "        cost += nd.sum(loss).asscalar()\n",
    "\n",
    "    return cost/N/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemN2N(None)\n",
    "model.collect_params().initialize(mx.init.Xavier(),ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemN2N(\n",
      "  (C): Embedding(19 -> 50, float32)\n",
      "  (B): Embedding(19 -> 50, float32)\n",
      "  (A): Embedding(19 -> 50, float32)\n",
      "  (W): Dense(50 -> 19, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.01})\n",
    "log_loss = []\n",
    "log_perp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memn2n0_ (\n",
       "  Parameter memn2n0_embedding0_weight (shape=(19, 50), dtype=<class 'numpy.float32'>)\n",
       "  Parameter memn2n0_embedding1_weight (shape=(19, 50), dtype=<class 'numpy.float32'>)\n",
       "  Parameter memn2n0_embedding2_weight (shape=(19, 50), dtype=<class 'numpy.float32'>)\n",
       "  Parameter memn2n0_dense0_weight (shape=(19, 50), dtype=<class 'numpy.float32'>)\n",
       "  Parameter memn2n0_dense0_bias (shape=(19,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs  = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity': 5.861413433301174, 'epoch': 0, 'learning_rate': 0.01}\n",
      "{'perplexity': 5.615093637421875, 'epoch': 1, 'learning_rate': 0.01}\n",
      "{'perplexity': 5.472005369722603, 'epoch': 2, 'learning_rate': 0.01}\n",
      "{'perplexity': 5.249945357915772, 'epoch': 3, 'learning_rate': 0.01}\n",
      "{'perplexity': 5.110687560808911, 'epoch': 4, 'learning_rate': 0.01}\n",
      "{'perplexity': 2.8746497849242862, 'epoch': 5, 'learning_rate': 0.01}\n",
      "{'perplexity': 2.181881302846117, 'epoch': 6, 'learning_rate': 0.01}\n",
      "{'perplexity': 1.868530939519261, 'epoch': 7, 'learning_rate': 0.01}\n",
      "{'perplexity': 1.7504710196665425, 'epoch': 8, 'learning_rate': 0.01}\n",
      "{'perplexity': 1.7880474000871802, 'epoch': 9, 'learning_rate': 0.01}\n",
      "update learning rate from 0.010 to 0.007\n",
      "{'perplexity': 1.795999823360926, 'epoch': 10, 'learning_rate': 0.006666666666666667}\n",
      "update learning rate from 0.007 to 0.004\n",
      "{'perplexity': 1.5709877500760883, 'epoch': 11, 'learning_rate': 0.0044444444444444444}\n",
      "{'perplexity': 1.572583452006037, 'epoch': 12, 'learning_rate': 0.0044444444444444444}\n",
      "update learning rate from 0.004 to 0.003\n",
      "{'perplexity': 1.4855911586392962, 'epoch': 13, 'learning_rate': 0.002962962962962963}\n",
      "{'perplexity': 1.4817647500155693, 'epoch': 14, 'learning_rate': 0.002962962962962963}\n",
      "{'perplexity': 1.4846817378614785, 'epoch': 15, 'learning_rate': 0.002962962962962963}\n",
      "update learning rate from 0.003 to 0.002\n",
      "{'perplexity': 1.4794742632618874, 'epoch': 16, 'learning_rate': 0.0019753086419753087}\n",
      "{'perplexity': 1.451990342634994, 'epoch': 17, 'learning_rate': 0.0019753086419753087}\n",
      "{'perplexity': 1.4449062433693256, 'epoch': 18, 'learning_rate': 0.0019753086419753087}\n",
      "{'perplexity': 1.4386153777211907, 'epoch': 19, 'learning_rate': 0.0019753086419753087}\n",
      "{'perplexity': 1.4460373963898705, 'epoch': 20, 'learning_rate': 0.0019753086419753087}\n",
      "update learning rate from 0.002 to 0.001\n",
      "{'perplexity': 1.4350522026615489, 'epoch': 21, 'learning_rate': 0.0013168724279835392}\n",
      "{'perplexity': 1.4489681258233964, 'epoch': 22, 'learning_rate': 0.0013168724279835392}\n",
      "update learning rate from 0.001 to 0.001\n",
      "{'perplexity': 1.4340364827986485, 'epoch': 23, 'learning_rate': 0.0008779149519890262}\n",
      "{'perplexity': 1.441295271219235, 'epoch': 24, 'learning_rate': 0.0008779149519890262}\n",
      "update learning rate from 0.001 to 0.001\n",
      "{'perplexity': 1.4336311310584688, 'epoch': 25, 'learning_rate': 0.0005852766346593508}\n",
      "{'perplexity': 1.4361942173051039, 'epoch': 26, 'learning_rate': 0.0005852766346593508}\n",
      "update learning rate from 0.001 to 0.000\n",
      "{'perplexity': 1.4344903426282267, 'epoch': 27, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4343193482458765, 'epoch': 28, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4342589066080658, 'epoch': 29, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.434074947170312, 'epoch': 30, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4339953270635029, 'epoch': 31, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4337777298713066, 'epoch': 32, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4336777026503895, 'epoch': 33, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4334254223568434, 'epoch': 34, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4333289224658228, 'epoch': 35, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4330454835970523, 'epoch': 36, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.432978262650267, 'epoch': 37, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4326692320751266, 'epoch': 38, 'learning_rate': 0.0003901844231062339}\n",
      "{'perplexity': 1.4326728186167237, 'epoch': 39, 'learning_rate': 0.0003901844231062339}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4320859834901192, 'epoch': 40, 'learning_rate': 0.00026012294873748923}\n",
      "{'perplexity': 1.4319908540707242, 'epoch': 41, 'learning_rate': 0.00026012294873748923}\n",
      "{'perplexity': 1.431900809163878, 'epoch': 42, 'learning_rate': 0.00026012294873748923}\n",
      "{'perplexity': 1.4319131419941862, 'epoch': 43, 'learning_rate': 0.00026012294873748923}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4316591668659877, 'epoch': 44, 'learning_rate': 0.00017341529915832616}\n",
      "{'perplexity': 1.4316673162418776, 'epoch': 45, 'learning_rate': 0.00017341529915832616}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4315101822645178, 'epoch': 46, 'learning_rate': 0.00011561019943888411}\n",
      "{'perplexity': 1.4315373584253803, 'epoch': 47, 'learning_rate': 0.00011561019943888411}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4314364209998964, 'epoch': 48, 'learning_rate': 7.70734662925894e-05}\n",
      "{'perplexity': 1.4314618893238904, 'epoch': 49, 'learning_rate': 7.70734662925894e-05}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.431396619652242, 'epoch': 50, 'learning_rate': 5.138231086172627e-05}\n",
      "{'perplexity': 1.4314159869436236, 'epoch': 51, 'learning_rate': 5.138231086172627e-05}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4313733280585799, 'epoch': 52, 'learning_rate': 3.425487390781751e-05}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.431345557807624, 'epoch': 53, 'learning_rate': 2.2836582605211673e-05}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4313275138319537, 'epoch': 54, 'learning_rate': 1.522438840347445e-05}\n",
      "update learning rate from 0.000 to 0.000\n",
      "{'perplexity': 1.4313154419846863, 'epoch': 55, 'learning_rate': 1.0149592268982966e-05}\n",
      "update learning rate from 0.000 to 0.000\n"
     ]
    }
   ],
   "source": [
    "epoch_log_loss = []\n",
    "import math\n",
    "for epoch in range(epochs):\n",
    "    dataiter = mx.io.NDArrayIter([inputs_train, queries_train], answers_train, batch_size, shuffle=False, last_batch_handle='discard')\n",
    "    \n",
    "    for batch in dataiter:\n",
    "        log_loss = []\n",
    "        train_loss = train(model, trainer, \n",
    "                           batch.data[0].as_in_context(ctx), \n",
    "                           batch.data[1].as_in_context(ctx),\n",
    "                           batch.label[0].as_in_context(ctx),\n",
    "                           batch_size)\n",
    "\n",
    "        # Logging\n",
    "        log_loss.append([train_loss])#, test_loss])\n",
    "    \n",
    "    epoch_log_loss.append([np.mean(log_loss)])\n",
    "    \n",
    "    state = { 'epoch': epoch, 'learning_rate': trainer.learning_rate, 'perplexity': math.exp(epoch_log_loss[epoch][0])}\n",
    "    print(state)\n",
    "    \n",
    "    lr_decay = 1.5\n",
    "    if (len(epoch_log_loss) > 1) and (epoch_log_loss[epoch][0] > epoch_log_loss[epoch-1][0] * 0.9999):\n",
    "        print ('update learning rate from %.3f to %.3f' % (trainer.learning_rate, trainer.learning_rate/lr_decay))\n",
    "        trainer.set_learning_rate(trainer.learning_rate / lr_decay)\n",
    "    if trainer.learning_rate < 1e-5: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = model(mx.nd.array(inputs_test, mx.cpu()), mx.nd.array(queries_test, mx.cpu()))\n",
    "pred_ = pred_.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3, 5, 6, 10, 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([np.argmax(i) for i in pred_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 5, 6, 10, 13}\n",
      "{2, 3, 5, 6, 10, 13}\n"
     ]
    }
   ],
   "source": [
    "print (set(np.argmax(i) for i in pred_))\n",
    "print (set(i for i in answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 10, 10, 10, 6, 5, 6, 2, 13]\n",
      "[6, 2, 10, 6, 10, 6, 5, 6, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "print ([np.argmax(i) for i in pred_[0:10]])\n",
    "print ([i for i in answers_test[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72599999999999998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([bool(np.argmax(i) == answers_test[idx:idx+1]) for idx, i in enumerate(pred_)])/len(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "## base rate\n",
    "print (1/len(set(i for i in answers_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john travelled to the hallway mary\n",
      "mary journeyed to the bathroom kitchen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "where is john\n",
      "hallway\n"
     ]
    }
   ],
   "source": [
    "for ido in test_data.data_story[idx:idx+1][0]:\n",
    "    print (\" \".join([word_re.get(int(i), '.') for i in ido if int(i) != 0]))\n",
    "    \n",
    "print (\" \".join([word_re.get(int(i), '.') for i in test_data.data_query[idx:idx+1][0] if int(i) != 0]))\n",
    "print (word_re.get(int(test_data.data_answer[idx:idx+1][0]), '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallway\n"
     ]
    }
   ],
   "source": [
    "print (word_re[ np.argmax(pred_[idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
