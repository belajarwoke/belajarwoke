{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#https://gluon-nlp.mxnet.io/examples/word_embedding/word_embedding.html#Applications-of-Word-Embeddings\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import babi\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qa1_single-supporting-fact_train.txt', 'qa1_single-supporting-fact_test.txt']\n"
     ]
    }
   ],
   "source": [
    "BABI_DIR = \"../data/tasks_1-20_v1-2/en\"\n",
    "EMB_DATA_DIR = \"../data/\"\n",
    "TASK_NBR = 1\n",
    "WORD2VEC_EMBED_SIZE = EMBED_HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "NBR_EPOCHS = 40\n",
    "\n",
    "train_file, test_file = babi.get_files_for_task(TASK_NBR, BABI_DIR)\n",
    "\n",
    "data_train = babi.get_stories(os.path.join(BABI_DIR, train_file))\n",
    "data_test = babi.get_stories(os.path.join(BABI_DIR, test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: Mary moved to the bathroom . John went to the hallway .\n",
      "QUERY: Where is Mary ?\n",
      "ANSWER: bathroom\n",
      "\n",
      "CONTEXT: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden .\n",
      "QUERY: Where is Daniel ?\n",
      "ANSWER: hallway\n",
      "\n",
      "CONTEXT: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden . John moved to the office . Sandra journeyed to the bathroom .\n",
      "QUERY: Where is Daniel ?\n",
      "ANSWER: hallway\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for context, query, answer in data_train:\n",
    "\n",
    "    print ('CONTEXT: '  + \" \".join(context))   \n",
    "    print ( 'QUERY: ' +  \" \".join(query))\n",
    "    print ( 'ANSWER: ' +  answer + '\\n')\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    if cnt == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size= 22\n",
      "story_maxlen= 66\n",
      "question_maxlen= 4\n",
      "(1000, 66) (1000, 4) (1000, 22)\n",
      "(1000, 66) (1000, 4) (1000, 22)\n"
     ]
    }
   ],
   "source": [
    "word2idx = babi.build_vocab([data_train, data_test])\n",
    "vocab_size = len(word2idx) + 1\n",
    "print(\"vocab_size=\", vocab_size)\n",
    "\n",
    "story_maxlen, question_maxlen = babi.get_maxlens([data_train, data_test])\n",
    "print(\"story_maxlen=\", story_maxlen)\n",
    "print(\"question_maxlen=\", question_maxlen)\n",
    "\n",
    "Xs_train, Xq_train, Y_train = babi.vectorize(data_train, word2idx, \n",
    "                                             story_maxlen, question_maxlen)\n",
    "Xs_test, Xq_test, Y_test = babi.vectorize(data_test, word2idx,\n",
    "                                          story_maxlen, question_maxlen)\n",
    "print(Xs_train.shape, Xq_train.shape, Y_train.shape)\n",
    "print(Xs_test.shape, Xq_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd, init\n",
    "from mxnet.gluon import Block, nn, rnn\n",
    "\n",
    "class MemN2N(Block):\n",
    "    def __init__(self, vocab_size=22, emb_dim=50, init_std=0.015, **kwargs):\n",
    "        super(MemN2N, self).__init__(**kwargs)\n",
    "        ## 고정\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        ## \n",
    "        self.init_std = init_std\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        with self.name_scope():\n",
    "             \n",
    "            # Embedding C (컨텍스트 벡터)\n",
    "            self.C = nn.Embedding(input_dim=self.vocab_size,\n",
    "                                                 output_dim=self.emb_dim, \n",
    "                                                 weight_initializer=init.Normal(self.init_std))\n",
    "            \n",
    "            # Embedding Q (쿼리 벡터)\n",
    "            self.B = nn.Embedding(input_dim=self.vocab_size, \n",
    "                                                     output_dim=self.emb_dim, \n",
    "                                                     weight_initializer=init.Normal(self.init_std))\n",
    "\n",
    "            # Final Predict\n",
    "            self.W_ = nn.Dense(self.vocab_size, weight_initializer=init.Normal(self.init_std))\n",
    "\n",
    "            self.C_encoder= rnn.LSTM(self.emb_dim,  bidirectional=True)\n",
    "            self.C_dropout = nn.Dropout(0.3)\n",
    "            self.Q_encoder= rnn.LSTM(self.emb_dim,  bidirectional=True)\n",
    "            self.Q_dropout = nn.Dropout(0.3)\n",
    "            \n",
    "    def forward(self, sentences, question):\n",
    "       \n",
    "        c_i = self.C(sentences)         \n",
    "        q_i = self.B(question)\n",
    "       \n",
    "        c_i_encode = self.C_encoder(c_i )\n",
    "        c_i_encode = self.C_dropout(c_i_encode)\n",
    "        q_i_encode = self.Q_encoder(q_i)\n",
    "        q_i_encode = self.Q_dropout(q_i_encode)\n",
    "\n",
    "        m_out = nd.batch_dot(c_i_encode, q_i_encode.swapaxes(1,2))\n",
    "        \n",
    "        #print (np.shape(m_out))\n",
    "        #print (np.shape(q_i))\n",
    "        \n",
    "        z = self.W_(m_out)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainer, softmax_cross_entropy, x_input, x_query, x_answers, batch_size):\n",
    "    max_grad_norm=40\n",
    "    \n",
    "    with autograd.record():\n",
    "        out = model(x_input, x_query)\n",
    "\n",
    "        loss = softmax_cross_entropy(out, x_answers)\n",
    "        loss.backward()\n",
    "        \n",
    "    grads = [i.grad() for i in model.collect_params().values()]\n",
    "    gluon.utils.clip_global_norm(grads, max_grad_norm)\n",
    "            \n",
    "    trainer.step(batch_size)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, epochs=3000, learning_rate=0.1):\n",
    "\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': learning_rate})#, 'clip_gradient':40, 'wd':1.01})\n",
    "    log_loss = []\n",
    "    log_perp = []\n",
    "    batch_size = 16\n",
    "    epoch_log_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        dataiter = mx.io.NDArrayIter([inputs_train, queries_train], answers_train, batch_size, shuffle=False, last_batch_handle='discard')\n",
    "\n",
    "        for batch in dataiter:\n",
    "            log_loss = []\n",
    "            train_loss = train(model, trainer, softmax_cross_entropy,\n",
    "                               batch.data[0].as_in_context(ctx), \n",
    "                               batch.data[1].as_in_context(ctx),\n",
    "                               batch.label[0].as_in_context(ctx),\n",
    "                               batch_size)\n",
    "\n",
    "            # Logging\n",
    "            log_loss.append([train_loss.asnumpy()/batch_size])#, test_loss])\n",
    "\n",
    "        epoch_log_loss.append([np.mean(log_loss)])\n",
    "\n",
    "        state = { 'epoch': epoch, 'learning_rate': trainer.learning_rate, 'perplexity': math.exp(epoch_log_loss[epoch][0])}\n",
    "        print(state)\n",
    "\n",
    "        #lr_decay = 1.01\n",
    "        #if (len(epoch_log_loss) > 1) and (epoch_log_loss[epoch][0] > epoch_log_loss[epoch-1][0] * 0.9999):\n",
    "          #  print ('update learning rate from %.3f to %.3f' % (trainer.learning_rate, trainer.learning_rate/lr_decay))\n",
    "           # trainer.set_learning_rate(trainer.learning_rate / lr_decay)\n",
    "        if trainer.learning_rate < 1e-5: \n",
    "            break\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/apache/incubator-mxnet/issues/9486\n",
    "#sym_c_data = mx.sym.Variable('data')\n",
    "#sym_q_data = mx.sym.Variable('data')\n",
    "#net = model(sym_c_data, sym_q_data)\n",
    "#viz = mx.viz.plot_network(net, title='lstm', save_format='png', shape={'data':(1,3,256,256)})\n",
    "#viz.render('images/sam')\n",
    "#from IPython.display import Image\n",
    "#Image(\"images/sam.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = nd.array(Xs_train)\n",
    "queries_train = nd.array(Xq_train)\n",
    "answers_train = nd.array([np.where(i == 1)[0][0] for i in Y_train])\n",
    "\n",
    "inputs_test = nd.array(Xs_test)\n",
    "queries_test = nd.array(Xq_test)\n",
    "answers_test = nd.array([np.where(i == 1)[0][0] for i in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "ctx = mx.gpu()\n",
    "model = MemN2N(vocab_size=vocab_size, emb_dim=50, init_std=0.015)\n",
    "model.collect_params().initialize(mx.init.Xavier(),ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module '_line_profiler' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'perplexity': 64.80204280860912, 'epoch': 0}\n",
      "{'learning_rate': 0.2, 'perplexity': 4.51971724392699, 'epoch': 1}\n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.75367 s\n",
      "File: <ipython-input-7-4e772d2b80f8>\n",
      "Function: train at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def train(model, trainer, softmax_cross_entropy, x_input, x_query, x_answers, batch_size):\n",
      "     2       124         59.0      0.5      0.0      max_grad_norm=40\n",
      "     3                                               \n",
      "     4       124       1137.0      9.2      0.0      with autograd.record():\n",
      "     5       124     379029.0   3056.7     13.8          out = model(x_input, x_query)\n",
      "     6                                           \n",
      "     7       124      24663.0    198.9      0.9          loss = softmax_cross_entropy(out, x_answers)\n",
      "     8       124      75660.0    610.2      2.7          loss.backward()\n",
      "     9                                                   \n",
      "    10       124      73357.0    591.6      2.7      grads = [i.grad() for i in model.collect_params().values()]\n",
      "    11       124    1979158.0  15961.0     71.9      gluon.utils.clip_global_norm(grads, max_grad_norm)\n",
      "    12                                                       \n",
      "    13       124     220477.0   1778.0      8.0      trainer.step(batch_size)\n",
      "    14                                           \n",
      "    15       124        126.0      1.0      0.0      return loss\n",
      "\n",
      "Total time: 2.79858 s\n",
      "File: <ipython-input-8-2f1e849cbb44>\n",
      "Function: model_train at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def model_train(model, epochs=3000, learning_rate=0.1):\n",
      "     2                                           \n",
      "     3         1         80.0     80.0      0.0      softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
      "     4         1        667.0    667.0      0.0      trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': learning_rate})#, 'clip_gradient':40, 'wd':1.01})\n",
      "     5         1          1.0      1.0      0.0      log_loss = []\n",
      "     6         1          0.0      0.0      0.0      log_perp = []\n",
      "     7         1          1.0      1.0      0.0      batch_size = 16\n",
      "     8         1          1.0      1.0      0.0      epoch_log_loss = []\n",
      "     9         3          4.0      1.3      0.0      for epoch in range(epochs):\n",
      "    10         2        151.0     75.5      0.0          dataiter = mx.io.NDArrayIter([inputs_train, queries_train], answers_train, batch_size, shuffle=False, last_batch_handle='discard')\n",
      "    11                                           \n",
      "    12       126      10051.0     79.8      0.4          for batch in dataiter:\n",
      "    13       124        136.0      1.1      0.0              log_loss = []\n",
      "    14       124         80.0      0.6      0.0              train_loss = train(model, trainer, softmax_cross_entropy,\n",
      "    15       124       7936.0     64.0      0.3                                 batch.data[0].as_in_context(ctx), \n",
      "    16       124       6009.0     48.5      0.2                                 batch.data[1].as_in_context(ctx),\n",
      "    17       124       5997.0     48.4      0.2                                 batch.label[0].as_in_context(ctx),\n",
      "    18       124    2757060.0  22234.4     98.5                                 batch_size)\n",
      "    19                                           \n",
      "    20                                                       # Logging\n",
      "    21       124      10174.0     82.0      0.4              log_loss.append([train_loss.asnumpy()/batch_size])#, test_loss])\n",
      "    22                                           \n",
      "    23         2         98.0     49.0      0.0          epoch_log_loss.append([np.mean(log_loss)])\n",
      "    24                                           \n",
      "    25         2         11.0      5.5      0.0          state = { 'epoch': epoch, 'learning_rate': trainer.learning_rate, 'perplexity': math.exp(epoch_log_loss[epoch][0])}\n",
      "    26         2        120.0     60.0      0.0          print(state)\n",
      "    27                                           \n",
      "    28                                                   #lr_decay = 1.01\n",
      "    29                                                   #if (len(epoch_log_loss) > 1) and (epoch_log_loss[epoch][0] > epoch_log_loss[epoch-1][0] * 0.9999):\n",
      "    30                                                     #  print ('update learning rate from %.3f to %.3f' % (trainer.learning_rate, trainer.learning_rate/lr_decay))\n",
      "    31                                                      # trainer.set_learning_rate(trainer.learning_rate / lr_decay)\n",
      "    32         2          6.0      3.0      0.0          if trainer.learning_rate < 1e-5: \n",
      "    33                                                       break\n",
      "    34                                                       \n",
      "    35         1          0.0      0.0      0.0      return model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profile = LineProfiler()\n",
    "lp_wrapper=profile(train)\n",
    "lp_wrapper=profile(model_train)\n",
    "lp_wrapper(model, epochs=2, learning_rate=0.2)\n",
    "profile.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'perplexity': 12.563814869331576, 'epoch': 0}\n",
      "{'learning_rate': 0.1, 'perplexity': 5.714457203734448, 'epoch': 1}\n",
      "{'learning_rate': 0.1, 'perplexity': 4.155200294894872, 'epoch': 2}\n",
      "{'learning_rate': 0.1, 'perplexity': 4.601557001660292, 'epoch': 3}\n",
      "{'learning_rate': 0.1, 'perplexity': 3.8386680205685435, 'epoch': 4}\n"
     ]
    }
   ],
   "source": [
    "model = model_train(model, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res():\n",
    "    import numpy as np\n",
    "    res = model(mx.nd.array(inputs_test, ctx), mx.nd.array(queries_test, ctx))\n",
    "    res = res.asnumpy()\n",
    "    answers_test = [np.where(i == 1)[0][0] for i in Y_test]\n",
    "    from collections import Counter\n",
    "\n",
    "    print (Counter(np.argmax(i) for i in res).items())\n",
    "    print (Counter(i for i in answers_test).items())\n",
    "    print ([np.argmax(i) for i in res[0:20]])\n",
    "    print ([i for i in answers_test[0:20]])\n",
    "\n",
    "    print ('ACC')\n",
    "    print (np.sum([bool(np.argmax(i) == answers_test[idx:idx+1]) for idx, i in enumerate(res)])/len(res))\n",
    "\n",
    "    print ('BASE')\n",
    "    print (1/len(set(i for i in answers_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUSTOM EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gensim\n",
    "WORD2VEC_BIN = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "WORD2VEC_EMBED_SIZE = 300\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    os.path.join(EMB_DATA_DIR, WORD2VEC_BIN), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.embedding.list_sources()\n",
    "#nlp.embedding.list_sources('fasttext')\n",
    "#nlp.embedding.list_sources('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_emb = nlp.embedding.create('fasttext', source='wiki.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(nlp.data.Counter(fasttext_emb.idx_to_token))\n",
    "vocab.set_embedding(fasttext_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((vocab_size, WORD2VEC_EMBED_SIZE))\n",
    "np.shape(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in word2idx.items():\n",
    "    print (word)\n",
    "    try:\n",
    "        embedding_weights[index, :] = vocab.embedding[word.lower()].asnumpy()\n",
    "    except KeyError:\n",
    "        pass  # keep as zero (not ideal, but what else can we do?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "def cos_sim(x, y):\n",
    "    return nd.dot(x, y) / (nd.norm(x) * nd.norm(y))\n",
    "\n",
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1)).reshape((-1,1))\n",
    "\n",
    "def get_knn(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+4, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove unknown and input tokens.\n",
    "    return vocab.to_tokens(indices[4:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(vocab.embedding['baby'], vocab.embedding['babies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(vocab.embedding['bathroom'], vocab.embedding['office'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemN2N(vocab_size=vocab_size, emb_dim=300, init_std=0.015)\n",
    "model.collect_params().initialize(mx.init.Xavier(),ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [i for i in list(model.collect_params().keys()) if 'embedding' in i]\n",
    "print (keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params()[list(keys)[0]].data().asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    model.collect_params()[key].set_data(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params()[list(keys)[0]].data().asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_train(model, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
