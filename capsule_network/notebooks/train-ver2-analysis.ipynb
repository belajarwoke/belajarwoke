{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np \n",
    "import mxnet as mx\n",
    "from tqdm import tqdm_notebook\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.loss import L2Loss\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "sys.path.append('../python/')\n",
    "from CapsuleNet import CapsuleNet, CapsuleMarginLoss\n",
    "from CapsuleLayer import CapsuleConv, CapsuleDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(1)\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'\n",
    "os.environ['MXNET_ENABLE_GPU_P2P'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/vision/datasets.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "/opt/venv/lib/python3.6/site-packages/mxnet/gluon/data/vision/datasets.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 128\n",
    "ctx = mx.cpu(0)\n",
    "scale_factor = 0.0005\n",
    "##############################################################\n",
    "###                    Load Dataset                        ###\n",
    "##############################################################\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True,\n",
    "                                transform=transform),batch_size,\n",
    "                                shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False,\n",
    "                                transform=transform),\n",
    "                                test_batch_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "for i, (data, label) in enumerate(train_data):\n",
    "    X = data\n",
    "    y = label\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "X = X.as_in_context(ctx)\n",
    "y = y.as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleConv(nn.HybridBlock):\n",
    "    def __init__(self,dim_vector, out_channels, kernel_size, \n",
    "                        strides=1,padding=0):\n",
    "        \"\"\"\n",
    "        dim_vector: 캡슐의 차원을 결정\n",
    "        out_channels: 서로 다른 capsule type의 갯수\n",
    "        kernel_size, strides: grid of capsule을 결정\n",
    "        \"\"\"\n",
    "        super(CapsuleConv, self).__init__()\n",
    "        # 논문에서는 0 - 8\n",
    "        self.capsules_index = ['dim_'+str(i) for i in range(dim_vector)]\n",
    "        print('capsule names = {}'.format(self.capsules_index))\n",
    "        for idx in self.capsules_index:\n",
    "            setattr(self, idx, nn.Conv2D(out_channels, \n",
    "                    kernel_size=kernel_size, strides=strides,\n",
    "                    padding=padding))\n",
    "\n",
    "    def squash(self, tensor):\n",
    "        \"\"\"Batch Squashing Function\n",
    "        \n",
    "        Args:\n",
    "            tensor : 5-D, (batch_size, num_channel,  height, width, dim_vector)\n",
    "            \n",
    "        Return:\n",
    "            tesnor_squached : 5-D, (batch_size, num_channel, height, width, dim_vector)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-9\n",
    "        tensor_l2norm = (tensor**2).sum(axis=-1).expand_dims(axis=-1)\n",
    "        scale_factor = tensor_l2norm / (1 + tensor_l2norm)\n",
    "        tensor_squashed = tensor * (scale_factor / (tensor_l2norm+epsilon)**0.5 )\n",
    "        return tensor_squashed\n",
    "    \n",
    "    def concact_vectors_in_list(self, vec_list, axis):\n",
    "        concat_vec = vec_list[0]\n",
    "        for i in range(1, len(vec_list)):\n",
    "            concat_vec = nd.concat(concat_vec, vec_list[i], dim=axis)\n",
    "\n",
    "        return concat_vec\n",
    "    def hybrid_forward(self,F, X):\n",
    "                    \n",
    "        outputs = [getattr(self,idx)(X).expand_dims(axis=-1) for idx in self.capsules_index]\n",
    "\n",
    "        outputs_cat = self.concact_vectors_in_list(outputs, axis=4)\n",
    "        outputs_squashed = self.squash(outputs_cat)\n",
    "        return outputs_squashed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create capsules step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution step (conv1 in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 20, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.HybridSequential()\n",
    "conv1.add(nn.Conv2D(256,kernel_size=9, strides=1, activation='relu'))\n",
    "conv1.initialize(ctx =ctx)\n",
    "conv1_res = conv1(X)\n",
    "conv1_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Width와 height가 줄어듬: 28 $\\rightarrow$ 20 ($\\because$ 28 - 9 + 1)\n",
    "* 32는 Batch size\n",
    "* 256은 filter의 갯수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule 생성\n",
    "* 8개의 원소를 가지는 convolution layer의 list를 만들어놓고, 이를 합하는 과정\n",
    "* ((6,6,8)이 채널의 갯수만큼) batch size만큼) 있도록 만들어주기 위해서\n",
    "> NOTE: dimension이 뒤로 가면 갈수록, 가장 최소의 단위라고 생각하면 됨.\n",
    "> * 8-dimension의 capsule이 \n",
    "> * $6\\times 6$만큼개가 모여서 같은 type의 capsule을 구성하고\n",
    "> * 32개의 서로 다른 type의 capsule group이 만들어지고\n",
    "> * 이런 capsule group이 총 32개의 example로 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (32, 32, 6, 6, 1)의 행렬을 8개 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsule names = ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_6', 'dim_7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 32, 6, 6, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = CapsuleConv(8, 32, 9, 2)\n",
    "cc.capsules_index\n",
    "cc.initialize(ctx = ctx)\n",
    "outputs = [getattr(cc,idx)(conv1_res).expand_dims(axis=-1) for idx in cc.capsules_index]\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: 256 channel에서 32 channel로 변환하는 convolution 2d에 대해..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1 = nn.Conv2D(channels = 32, kernel_size = 9, strides = 2)\n",
    "cc1.initialize(ctx = ctx)\n",
    "cc1_res = cc1(conv1_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다음과 같이 256개의 (9, 9) kernel로 1개의 output channel로 만든 다음\n",
    "* 이 과정을 32번 반복하는 것\n",
    "* 그래서, 실제 가지는 서로 다른 weight의 개수는 $9 \\times 9 \\times 256 \\times 32 = 663552$개임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Parameter.data of Parameter conv9_weight (shape=(32, 256, 9, 9), dtype=<class 'numpy.float32'>)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc1.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그래서 얻어지는 결과물의 크기는 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 6, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc1_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (32, 32, 6, 6, 8)의 data로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 마지막 dimension을 expand한 후에 마지막 axis에 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 6, 6, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_cat = cc.concact_vectors_in_list(outputs, axis=4)\n",
    "outputs_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_squashed = cc.squash(outputs_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 6, 6, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squashing하기 전과 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_squashed = outputs_squashed.reshape((-1, 8))\n",
    "v_output = outputs_cat.reshape((-1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of output = \n",
      "[3.660471]\n",
      "<NDArray 1 @cpu(0)>, length of squashed output = \n",
      "[0.6168993]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "outputs_len = (v_output[0] ** 2).sum(axis = -1)\n",
    "squashed_len = (v_squashed[0] ** 2).sum(axis = -1)\n",
    "print('length of output = {}, length of squashed output = {}'\\\n",
    "      .format(outputs_len, squashed_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner product of two vectors = [1.]\n"
     ]
    }
   ],
   "source": [
    "def length(a):\n",
    "    return nd.sqrt((a**2).sum(axis = -1))\n",
    "d1 = outputs_len/length(outputs_len)\n",
    "d2 = squashed_len/length(squashed_len)\n",
    "aa = nd.dot(d1, d2)\n",
    "print('inner product of two vectors = {}'.format(nd.dot(d1, d2).asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary capsule looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsule names = ['dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_6', 'dim_7']\n"
     ]
    }
   ],
   "source": [
    "capsuleConv = nn.HybridSequential()\n",
    "capsuleConv.add(CapsuleConv(8, 32, 9, 2))\n",
    "capsuleConv.initialize(ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary = capsuleConv(conv1_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 6, 6, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement\n",
    "> maxpooling을 대체하는 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0은 차원을 그대로 유지하라는 의미\n",
    "* 1은 차원을 1로 유지하라는 의미\n",
    "* -1은 차원을 알아서 찾으라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1152, 1, 1, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_primary = primary.reshape((0, -1, 1, 1, 0))\n",
    "_primary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전 단계에는 총 1152개의 capsule이 존재하고\n",
    "* _primary를 2번 축으로 10배 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tile = nd.tile(_primary, reps = (1, 1, 10, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8차원 capsule에서 16차원 capsule로..\n",
    "  * 최적의 16차원 capsule을 만들기 위해 routing by agreement 방법을 사용합니다.\n",
    "  * routing by agreement에는 capsule의 가중치, $c_{ij}$, 결정해야 하는데, 최초의 prior는 0을 사용합니다. (logit값이므로, 같은 확률로 이전 단계의 capsule이 다음 단계의 capsule에 영향을 줌을 의미합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_capsules_prev = _primary.shape[1]\n",
    "batch_size = _primary.shape[0]\n",
    "out_channels = 10 # output의 class 숫자\n",
    "\n",
    "routing_weight = nd.random_normal(\n",
    "                    shape=(1,\n",
    "                    num_capsules_prev, out_channels,\n",
    "                    8, 16)\n",
    "               , name='routing_weight').as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 8, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tile = nd.tile(routing_weight, reps=(batch_size, 1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination_3d = nd.batch_dot(\n",
    "                X_tile.reshape((-1, X_tile.shape[-2], X_tile.shape[-1])), \n",
    "                W_tile.reshape((-1, W_tile.shape[-2], W_tile.shape[-1])))\n",
    "linear_combination = linear_combination_3d.reshape((batch_size,\n",
    "                                num_capsules_prev, out_channels,\n",
    "                                1, 16))\n",
    "# b_ij (1, 1152, 10, 1, 1)\n",
    "priors = nd.zeros((1, num_capsules_prev, out_channels,1,1), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1152, 10, 1, 16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_combination.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 1, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing by agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1번의 iteration만..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prior = nd.softmax(priors, axis=2) # on num_capsule dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 1, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[1.]]\n",
       "\n",
       "  [[1.]]\n",
       "\n",
       "  [[1.]]\n",
       "\n",
       "  ...\n",
       "\n",
       "  [[1.]]\n",
       "\n",
       "  [[1.]]\n",
       "\n",
       "  [[1.]]]]\n",
       "<NDArray 1x1152x1x1 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_prior.sum(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =  softmax_prior * linear_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1152, 10, 1, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sum = output.sum(axis=1, keepdims=True) # s_J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: RoutingAlgorithm-line 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_squashed = squash(output_sum) # v_J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: RoutingAlgorithm-line 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tile = nd.tile(output_squashed, reps=(1,self.num_capsules_prev,1,1,1))\n",
    "U_times_v = nd.batch_dot(linear_combination.reshape((-1, 1, self.dim_vector)),\n",
    "                output_tile.reshape((-1, 1, self.dim_vector)),\n",
    "                transpose_b =True)\n",
    "U_times_v = U_times_v.reshape((self.batch_size, self.num_capsules_prev,\n",
    "                self.out_channels, 1, 1))\n",
    "priors = priors + U_times_v.sum(axis=0).expand_dims(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridSequential(\n",
       "  (0): HybridSequential(\n",
       "    (0): Conv2D(1 -> 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "  )\n",
       "  (1): HybridSequential(\n",
       "    (0): CapsuleConv(\n",
       "      (dim_0): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_1): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_2): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_3): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_4): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_5): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_6): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (dim_7): Conv2D(256 -> 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (2): HybridSequential(\n",
       "    (0): CapsuleDense(\n",
       "    \n",
       "    )\n",
       "  )\n",
       "  (3): HybridSequential(\n",
       "    (0): Dense(16 -> 512, Activation(relu))\n",
       "    (1): Dense(512 -> 1024, Activation(relu))\n",
       "    (2): Dense(1024 -> 784, Activation(sigmoid))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsule_net.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958c3350437747ce93c452ae39ffe51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-47a25db90dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# capsule_net.hybridize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m train(capsule_net, epochs,ctx,train_data,test_data, margin_loss,\n\u001b[0;32m---> 11\u001b[0;31m             reconstructions_loss, batch_size, scale_factor)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d3e56015da06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, ctx, train_data, test_data, margin_loss, reconstructions_loss, batch_size, scale_factor)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:{}, TrainLoss:{:.5f}, TestAcc:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert to static graph for speedup\n",
    "# capsule_net.hybridize()\n",
    "train(capsule_net, epochs,ctx,train_data,test_data, margin_loss,\n",
    "            reconstructions_loss, batch_size, scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
